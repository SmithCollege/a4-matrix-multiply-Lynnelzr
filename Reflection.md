The timing results in the table highlight the performance differences between the CPU, basic GPU, tiled GPU, and cuBLAS implementations of matrix multiplication for matrix sizes 256, 512, and 1024\. The CPU implementation is significantly slower than all GPU approaches, especially as the matrix size increases. For instance, at size 1024, the CPU takes 4.845 seconds, while the GPU approaches complete within milliseconds. The basic GPU implementation is faster than the CPU but still less efficient than the tiled and cuBLAS versions, which optimize memory access patterns. The tiled GPU version shows noticeable improvement over the basic GPU version, particularly for larger matrices, as it utilizes shared memory more effectively to reduce the number of global memory accesses.

Compared to cuBLAS, which is a highly optimized library for linear algebra operations, the tiled GPU implementation performs relatively well but still cannot match the efficiency of cuBLAS. cuBLAS achieves the fastest times across all matrix sizes, reflecting its highly optimized and low-level implementations that leverage CUDA's full potential.

What went well with this assignment was the successful implementation of different matrix multiplication techniques and the collection of performance data that provides insights into GPU optimizations. However, implementing and tuning the tiled GPU version was challenging, particularly in managing shared memory and synchronization to ensure correct and efficient computations. If approaching this again, I would consider experimenting with different tile sizes to observe how they impact performance to better understand performance bottlenecks in each GPU implementation.  
